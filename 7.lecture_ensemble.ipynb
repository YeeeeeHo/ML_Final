{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927b05a8",
   "metadata": {},
   "source": [
    "\n",
    "# 앙상블 학습 (Ensemble Learning)\n",
    "여러 개의 분류기를 생성하고 예측을 결합하여 보다 정확한 최종 예측을 도출하는 기법.\n",
    "\n",
    "### 주요 기법\n",
    "1. **보팅 (Voting)**: 여러 분류기의 예측 결과를 결합.\n",
    "2. **배깅 (Bagging)**: 부트스트랩 샘플링을 사용하여 여러 모델 학습.\n",
    "3. **부스팅 (Boosting)**: 순차적으로 학습하여 예측 오류를 개선.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934bbe4",
   "metadata": {},
   "source": [
    "\n",
    "# 보팅 (Voting)\n",
    "### 설명\n",
    "- **하드 보팅 (Hard Voting)**: 다수결 원칙을 사용하여 최종 결과 도출.\n",
    "- **소프트 보팅 (Soft Voting)**: 각 분류기의 레이블 결정 확률을 평균하여 최종 결과 도출.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc3ec95",
   "metadata": {},
   "source": [
    "\n",
    "# 랜덤 포레스트 (Random Forest)\n",
    "### 설명\n",
    "- 배깅 방식을 사용한 대표적인 앙상블 학습 알고리즘.\n",
    "- 여러 개의 결정 트리 분류기가 배깅 방식으로 학습하고 보팅을 통해 예측 결정.\n",
    "\n",
    "### 주요 특징\n",
    "- 빠른 속도와 높은 예측 성능.\n",
    "- 데이터 샘플링 시 부트스트랩 방식을 사용.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3beaee",
   "metadata": {},
   "source": [
    "\n",
    "# 부스팅 (Boosting)\n",
    "### 설명\n",
    "- 여러 개의 약한 학습기를 순차적으로 학습.\n",
    "- 잘못 예측된 데이터에 가중치를 부여하여 오류를 개선.\n",
    "\n",
    "### 주요 알고리즘\n",
    "1. **AdaBoost**: 잘못된 예측에 가중치를 부여.\n",
    "2. **GBM (Gradient Boosting Machine)**: 잔차를 최소화.\n",
    "3. **XGBoost**: GBM의 단점을 보완.\n",
    "4. **LightGBM**: 빠른 학습 속도와 적은 메모리 사용.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6681ab4",
   "metadata": {},
   "source": [
    "\n",
    "# 오버샘플링 및 언더샘플링\n",
    "### 데이터 불균형 문제 해결 기법\n",
    "- **언더샘플링 (Undersampling)**: 다수 클래스 데이터를 줄임.\n",
    "- **오버샘플링 (Oversampling)**: 소수 클래스 데이터를 증식.\n",
    "\n",
    "### SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "- 소수 클래스 데이터에서 임의로 하나를 선택하고, 가까운 데이터와의 직선 상에서 새로운 데이터를 생성.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
