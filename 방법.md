# classification 방법
데이터셋을 처음 받아 분류(Classification) 모델을 수행하기 위한 전체 프로세스를 아래 단계별로 자세히 설명합니다. 각각의 단계는 데이터 특성과 목표에 따라 조정할 수 있습니다.

1. 데이터 로드 및 탐색

1.1 데이터 로드

	•	CSV 파일 등을 로드하여 pandas 데이터프레임으로 저장.

import pandas as pd

# 데이터 로드
file_path = 'your_dataset.csv'
data = pd.read_csv(file_path)

# 데이터의 구조 확인
print(data.head())  # 처음 5행 확인
print(data.info())  # 열 정보 및 데이터 타입 확인
print(data.describe())  # 수치형 열의 통계 요약

1.2 결측값 확인

	•	결측값을 처리해야 데이터의 품질을 보장할 수 있습니다.

# 결측값 확인
print(data.isnull().sum())

1.3 타겟 변수 확인

	•	타겟 변수(분류 대상)가 **이진(0/1)**인지 다중 클래스인지 확인.
	•	타겟 변수의 분포를 확인하여 데이터가 불균형한지 체크.

# 타겟 변수 분포
target = 'target_column_name'  # 타겟 변수명
print(data[target].value_counts())

2. 데이터 전처리

2.1 결측값 처리

	•	결측값이 있는 열이나 행을 처리.
	•	수치형 데이터: 평균, 중앙값으로 대체.
	•	범주형 데이터: 최빈값으로 대체.

# 수치형 결측값 대체 (중앙값)
data['numeric_column'].fillna(data['numeric_column'].median(), inplace=True)

# 범주형 결측값 대체 (최빈값)
data['categorical_column'].fillna(data['categorical_column'].mode()[0], inplace=True)

2.2 불필요한 열 제거

	•	모델에 의미가 없거나, 타겟 변수 예측에 직접적으로 연관된 열(예: ID, 이름)을 제거.

# 불필요한 열 삭제
data.drop(['unnecessary_column1', 'unnecessary_column2'], axis=1, inplace=True)

2.3 범주형 데이터 처리

	•	라벨 인코딩(Label Encoding): 순서가 있는 범주형 데이터.
	•	원-핫 인코딩(One-Hot Encoding): 순서가 없는 범주형 데이터.

from sklearn.preprocessing import LabelEncoder

# 라벨 인코딩
label_encoder = LabelEncoder()
data['encoded_column'] = label_encoder.fit_transform(data['categorical_column'])

# 원-핫 인코딩
data = pd.get_dummies(data, columns=['categorical_column'], drop_first=True)

2.4 스케일링

	•	필요 이유: 대부분의 머신러닝 모델은 특성 간의 값 범위가 크면 성능이 저하될 수 있음.
	•	적용 대상: 수치형 데이터.

from sklearn.preprocessing import StandardScaler

# 수치형 열 스케일링
scaler = StandardScaler()
data[['numeric_column1', 'numeric_column2']] = scaler.fit_transform(data[['numeric_column1', 'numeric_column2']])

3. 데이터 분리

	•	데이터를 **훈련(Train)**과 테스트(Test) 세트로 나눕니다.

from sklearn.model_selection import train_test_split

# 타겟 변수와 독립 변수 분리
X = data.drop('target_column_name', axis=1)  # 특성
y = data['target_column_name']  # 타겟 변수

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

4. 분류 모델 구축

4.1 모델 선택

	•	주요 분류 모델:
	•	로지스틱 회귀 (Logistic Regression)
	•	의사결정 나무 (Decision Tree)
	•	랜덤 포레스트 (Random Forest)
	•	서포트 벡터 머신 (SVM)
	•	XGBoost, LightGBM

4.2 모델 학습

from sklearn.ensemble import RandomForestClassifier

# 모델 생성 및 학습
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

4.3 모델 평가

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 예측
y_pred = model.predict(X_test)

# 평가
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

4.4 중요 변수 확인 (특성 중요도)

# 특성 중요도
import matplotlib.pyplot as plt

feature_importances = model.feature_importances_
features = X_train.columns

plt.figure(figsize=(10, 6))
plt.barh(features, feature_importances)
plt.title("Feature Importances")
plt.xlabel("Importance")
plt.ylabel("Features")
plt.show()

5. 모델 튜닝

5.1 하이퍼파라미터 튜닝

	•	GridSearchCV 또는 RandomizedSearchCV를 사용하여 모델 성능 최적화.

from sklearn.model_selection import GridSearchCV

# 하이퍼파라미터 설정
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# GridSearchCV 실행
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

6. 결과 시각화

	•	Confusion Matrix 시각화:

import seaborn as sns
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

7. 결과 분석 및 결론

	•	모델 성능이 적절한지 확인 (예: 정확도, F1 점수 등).
	•	타겟 변수의 불균형 여부 확인.
	•	모델이 과적합되었는지 테스트 세트 성능을 통해 판단.

요약

	1.	데이터 로드 및 탐색: 데이터 구조와 결측값 파악.
	2.	데이터 전처리: 결측값 처리, 범주형 데이터 인코딩, 스케일링.
	3.	데이터 분리: 훈련 데이터와 테스트 데이터 분리.
	4.	모델 학습 및 평가: 모델 학습, 예측, 성능 평가.
	5.	모델 튜닝: 하이퍼파라미터 최적화로 성능 향상.
	6.	시각화 및 분석: 혼동 행렬과 특성 중요도 시각화.

## 데이터 전처리 스케일링 및 범주형 데이터

데이터 전처리에서 범주형 데이터 처리와 스케일링은 서로 다른 목적과 데이터를 대상으로 합니다. 각각 어떤 데이터에 대해 적용해야 하는지 예시와 함께 설명하겠습니다.

1. 범주형 데이터 처리

적용 대상

	•	범주형 데이터(Categorical Data): 값이 특정 범주(카테고리)를 나타내는 데이터.
	•	데이터가 텍스트형이거나, 숫자형이라도 범주적 의미를 가진다면 처리해야 합니다.
	•	예: Gender, Color, Region 등.

처리 방법

1) 라벨 인코딩 (Label Encoding)

	•	범주형 데이터가 순서가 있는 경우 사용.
	•	문자열 데이터를 정수로 변환.
	•	예: ['Low', 'Medium', 'High'] → [0, 1, 2].

2) 원-핫 인코딩 (One-Hot Encoding)

	•	범주형 데이터가 순서가 없는 경우 사용.
	•	각 범주를 새로운 열로 만들어 이진값(0 또는 1)으로 표시.
	•	예: ['Red', 'Green', 'Blue'] →

Red   Green   Blue
 1      0       0
 0      1       0
 0      0       1

예시

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# 데이터 예시
data = pd.DataFrame({
    'Gender': ['Male', 'Female', 'Female', 'Male'],  # 순서 없음
    'Education': ['High School', 'Bachelor', 'Master', 'PhD']  # 순서 있음
})

# 라벨 인코딩 (Education)
label_encoder = LabelEncoder()
data['Education_encoded'] = label_encoder.fit_transform(data['Education'])

# 원-핫 인코딩 (Gender)
data = pd.get_dummies(data, columns=['Gender'], drop_first=True)

print(data)

결과

Education	Gender	Education_encoded	Gender_Male
High School	Male	1	1
Bachelor	Female	0	0
Master	Female	2	0
PhD	Male	3	1

처리 대상과 비처리 대상

데이터 열	처리 여부	이유
Gender	원-핫 인코딩 필요	순서가 없고 범주형 데이터.
Education	라벨 인코딩 필요	순서가 있는 범주형 데이터.
Age	처리 불필요	수치형 데이터.
Income	처리 불필요	수치형 데이터.

2. 스케일링

적용 대상

	•	수치형 데이터(Numerical Data): 연속적인 값을 가지며, 값의 크기가 의미 있는 데이터.
	•	스케일링은 머신러닝 모델에서 특성 간 크기 차이가 모델 성능에 영향을 줄 때 필요.
	•	예: Age, Income, Height, Weight 등.

처리 방법

1) 표준화 (Standardization)

	•	데이터를 평균 0, 표준편차 1로 변환.
	•	사용 모델: SVM, KNN, PCA 등 거리 기반 알고리즘.
	•	수식:
￼

2) 정규화 (Normalization)

	•	데이터를 0~1 범위로 변환.
	•	사용 모델: 신경망, 로지스틱 회귀 등.
	•	수식:
￼

예시

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# 데이터 예시
data = pd.DataFrame({
    'Age': [25, 30, 35, 40],
    'Income': [50000, 60000, 75000, 100000]
})

# 표준화
scaler = StandardScaler()
data[['Age_standardized', 'Income_standardized']] = scaler.fit_transform(data[['Age', 'Income']])

# 정규화
normalizer = MinMaxScaler()
data[['Age_normalized', 'Income_normalized']] = normalizer.fit_transform(data[['Age', 'Income']])

print(data)

결과

Age	Income	Age_standardized	Income_standardized	Age_normalized	Income_normalized
25	50000	-1.34	-1.07	0.00	0.00
30	60000	-0.45	-0.56	0.25	0.20
35	75000	0.45	0.06	0.50	0.50
40	100000	1.34	1.57	1.00	1.00

처리 대상과 비처리 대상

데이터 열	처리 여부	이유
Age	스케일링 필요	수치형 데이터. 모델 학습에 스케일링 필요.
Income	스케일링 필요	수치형 데이터. 값 범위가 큼.
Gender	처리 불필요	범주형 데이터. 스케일링 대상 아님.
Education	처리 불필요	범주형 데이터. 스케일링 대상 아님.

결론

범주형 데이터 처리

	•	텍스트형 데이터: 반드시 처리 필요.
	•	숫자로 표현된 범주형 데이터: 범주적 의미가 있다면 처리.

스케일링

	•	수치형 데이터에 적용.
	•	특성 값의 범위 차이가 클수록 스케일링이 중요.
	•	거리 기반 알고리즘(SVM, KNN, PCA 등)에서 반드시 필요.

상관계수 히트맵은 분류(Classification), 회귀(Regression), 군집화(Clustering) 모두에서 사용할 수 있습니다. 다만, 각 문제에서 사용 목적과 필요성이 다릅니다. 아래에서 각각의 경우를 설명하겠습니다.

1. 분류(Classification)에서 상관계수 히트맵

목적

	•	분류에서 타겟 변수와 독립 변수 간의 상관관계를 분석.
	•	상관관계가 높은 변수는 중요한 입력(feature)이 될 가능성이 큽니다.
	•	다중공선성 문제를 발견하고 제거할 변수를 선택할 수도 있음.

사용 예시

	•	이진 분류(0/1) 문제에서 독립 변수와 타겟 변수 간의 상관성을 확인.
	•	입력 변수 간의 높은 상관관계를 확인해 다중공선성 문제 해결.

주의

	•	분류의 타겟 변수(범주형)는 상관계수를 계산할 수 없으므로, 숫자로 변환(예: 라벨 인코딩)해야 합니다.

2. 회귀(Regression)에서 상관계수 히트맵

목적

	•	회귀 분석에서는 타겟 변수와 독립 변수 간의 상관관계를 분석하는 것이 매우 중요.
	•	상관계수가 높은 변수는 타겟 변수 예측에 중요한 역할을 할 가능성이 높음.
	•	다중공선성 문제를 탐지하여 불필요한 변수를 제거하거나 변형(PCA 등)할 수 있음.

사용 예시

	•	집값 예측(SalePrice)에서 GrLivArea와 같은 변수와 강한 상관관계를 확인.
	•	입력 변수들 간의 관계를 파악해 모델의 안정성을 높임.

주의

	•	상관계수만으로 변수 선택을 모두 결정하면 안 되고, 변수의 실제 중요도를 추가적으로 확인해야 함.

3. 군집화(Clustering)에서 상관계수 히트맵

목적

	•	군집화는 타겟 변수가 없지만, 입력 변수 간의 상관관계를 분석하여 중요한 정보를 얻을 수 있음.
	•	상관관계가 높은 변수들끼리 군집화에 비슷한 영향을 줄 수 있으므로, 변수를 줄이거나 조합(PCA 등)할 수 있음.

사용 예시

	•	군집화 전에 상관계수를 사용해 상관관계가 높은 변수 중 일부를 제거하거나 축소.
	•	변수 간의 상관성을 파악해 군집화 결과를 더 잘 해석할 수 있음.

주의

	•	상관관계는 변수 간의 선형적 관계만 나타내므로, 비선형적 관계를 놓칠 수 있음.

결론

	•	분류, 회귀, 군집화 모두에서 상관계수 히트맵을 사용할 수 있습니다.
	•	분류: 타겟 변수와 독립 변수의 관계를 확인.
	•	회귀: 타겟 변수와 독립 변수의 관계 + 다중공선성 문제 탐지.
	•	군집화: 입력 변수 간의 관계를 확인하여 군집화 결과 개선.

언제 유용하지 않은가?

	•	데이터가 대부분 범주형이고 숫자로 변환하더라도 상관계수가 의미를 가지지 않는 경우.
	•	데이터 간 관계가 비선형적일 때, 상관계수만으로는 충분하지 않을 수 있음.

추가 팁

상관계수 히트맵은 데이터를 이해하는 첫 단계에서 특히 유용합니다. 그러나 상관계수만으로 모델링에 필요한 모든 정보를 얻을 수는 없으므로, 다른 분석 도구와 함께 사용하는 것이 좋습니다.

데이터에서 스케일링과 범주형 데이터 처리는 각각 다른 목적과 필요성을 가지며, 특정 열에 대해 사용하는 이유를 이해하는 것이 중요합니다. 이를 명확히 하기 위해 언제, 왜, 어떤 열에 사용하는지를 설명하겠습니다.

1. 스케일링

목적

	•	**스케일링(Scaling)**은 수치형 데이터의 범위를 조정하는 과정입니다.
	•	머신러닝 모델이 입력 데이터의 크기 차이에 민감하기 때문에 스케일링이 필요합니다.
	•	모델이 숫자의 상대적 크기 대신 값의 패턴에 집중하도록 도와줍니다.

어떤 열에 사용하는가?

	•	수치형 데이터(Continuous or Numerical) 열에 사용합니다.
	•	예: 나이(Age), 소득(Income), 면적(Area), 거리(Distance) 등.

왜 필요한가?

	1.	값의 범위 차이가 큰 경우
	•	예: 한 열은 0~1 범위(확률), 다른 열은 0~1000 범위(면적)일 때, 값의 차이 때문에 모델이 특정 열에 더 큰 가중치를 부여.
	•	예: Age는 20~80, Income은 10,000~100,000일 때 스케일링 필요.
	2.	거리 기반 모델에서 중요
	•	K-Means, KNN, SVM, PCA와 같은 알고리즘은 거리 계산에 의존하기 때문에 스케일링이 필수.
	•	스케일이 다르면 특정 변수가 거리 계산에 더 큰 영향을 미침.
	3.	훈련 속도와 성능 개선
	•	값의 범위가 일정하면 모델의 수렴 속도가 빨라지고 성능이 안정적.

스케일링 방법

1) 표준화 (Standardization)

	•	데이터를 평균 0, 표준편차 1로 변환.
	•	데이터가 정규분포를 따르는 경우 적합.

from sklearn.preprocessing import StandardScaler

# 표준화 적용
scaler = StandardScaler()
data['scaled_column'] = scaler.fit_transform(data[['original_column']])

2) 정규화 (Normalization)

	•	데이터를 0~1 사이로 변환.
	•	데이터가 특정 범위 내에서 상대적인 크기를 유지해야 할 때 적합.

from sklearn.preprocessing import MinMaxScaler

# 정규화 적용
scaler = MinMaxScaler()
data['normalized_column'] = scaler.fit_transform(data[['original_column']])

예제

데이터 예시

Age	Income	Gender
25	50000	Male
30	75000	Female
35	120000	Female

스케일링 적용 전후

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# 표준화
scaler = StandardScaler()
data[['Age_scaled', 'Income_scaled']] = scaler.fit_transform(data[['Age', 'Income']])

# 정규화
normalizer = MinMaxScaler()
data[['Age_normalized', 'Income_normalized']] = normalizer.fit_transform(data[['Age', 'Income']])

Age_scaled	Income_scaled	Age_normalized	Income_normalized
-1.2247	-1.1355	0.00	0.00
0.0000	-0.1622	0.50	0.33
1.2247	1.2978	1.00	1.00

2. 범주형 데이터 처리

목적

	•	**범주형 데이터(Categorical Data)**는 머신러닝 모델에서 처리할 수 있도록 숫자로 변환해야 합니다.
	•	모델은 텍스트(문자)를 직접 처리할 수 없으며, 숫자 형태의 데이터만 학습 가능.

어떤 열에 사용하는가?

	•	범주형 열(Categorical Columns):
	•	값이 특정 범주(카테고리)를 나타냄.
	•	예: 성별(Gender: Male/Female), 도시(City: Seoul/New York), 색상(Color: Red/Blue/Green) 등.

왜 필요한가?

	1.	모델 입력을 숫자로 변환
	•	머신러닝 모델은 문자열 데이터(Male, Female)를 이해할 수 없으므로, 숫자로 변환해야 함.
	2.	범주 간 관계를 명확히 표현
	•	예: Low, Medium, High는 순서형 데이터이므로 숫자로 변환(라벨 인코딩).
	•	Red, Blue, Green처럼 순서가 없는 경우에는 원-핫 인코딩.

범주형 데이터 처리 방법

1) 라벨 인코딩 (Label Encoding)

	•	**순서형 데이터(Ordinal Data)**에 적합.
	•	예: Low → 0, Medium → 1, High → 2.

from sklearn.preprocessing import LabelEncoder

# 라벨 인코딩
label_encoder = LabelEncoder()
data['Gender_encoded'] = label_encoder.fit_transform(data['Gender'])

2) 원-핫 인코딩 (One-Hot Encoding)

	•	**순서가 없는 데이터(Nominal Data)**에 적합.
	•	각 범주를 열로 만들고 이진값(0 또는 1)으로 표현.

# 원-핫 인코딩
data = pd.get_dummies(data, columns=['Gender'], drop_first=True)

예제

데이터 예시

Gender	City
Male	Seoul
Female	New York
Female	Tokyo

라벨 인코딩 결과

Gender_encoded	City_encoded
1	2
0	0
0	1

원-핫 인코딩 결과

Gender_Male	City_New York	City_Tokyo
1	0	0
0	1	0
0	0	1

3. 정리

작업	적용 대상	사용 이유
스케일링	수치형 데이터 (Age, Income)	데이터 범위 차이를 줄이고, 거리 기반 알고리즘(SVM, KNN 등)의 성능을 개선.
범주형 처리	범주형 데이터 (Gender, City)	텍스트 데이터를 숫자로 변환하여 모델 입력으로 사용 가능하게 함.

스케일링은 수치형 데이터에만 적용, 범주형 데이터 처리는 범주형 데이터에만 적용해야 하며, 올바르게 구분하지 않으면 데이터 전처리가 실패할 수 있습니다. 데이터를 잘 분석하고 변수의 특성을 이해한 후 적절히 전처리를 적용하세요!

