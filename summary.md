# ML_lecture

# 분류 모델(지도 학습)
## 1. 나이브 베이즈 (Naive Bayes)
- 텍스트 분류: 스팸 이메일 분류, 감성 분석 등.
- 빠른 예측이 필요한 경우: 계산이 간단하여 대규모 데이터에서도 효율적.
- 데이터가 조건부 독립성을 대략적으로 만족하는 경우.
- ex)뉴스 기사 분류 (스포츠, 정치 등), 이메일 스팸 필터링, 고객 리뷰의 긍정/부정 분류

## 2. 로지스틱 회귀 (Logistic Regression)
- 이진 분류가 필요한 경우: 종속 변수가 두 가지 상태(0/1, Yes/No)일 때.
- 선형적으로 분리 가능한 데이터에서 간단하고 해석 가능한 모델을 원할 때.
- 특성의 영향력을 파악하고자 할 때.
- ex)환자의 특정 질병 발병 가능성 예측, 고객의 제품 구매 여부 예측, 금융 사기 탐지

## 3. 결정 트리 (Decision Tree)
- 직관적이고 해석이 쉬운 모델이 필요한 경우.
- 비선형 데이터를 처리해야 할 때.
- 범주형 데이터와 연속형 데이터를 혼합하여 처리할 때.
- ex)고객 분류: 고객 이탈 여부 예측, 의료 진단: 증상을 기준으로 질병 분류, 대출 승인 여부 결정

## 4. 서포트 벡터 머신 (Support Vector Machine)
- 데이터가 고차원(특성의 수가 많음)인 경우.
- 선형 또는 비선형 분리가 필요한 경우: 커널 함수를 사용하여 비선형 분리 가능.
- 소규모 데이터 세트에서 높은 분류 정확도를 원할 때.
- ex)이미지 분류: 얼굴 인식, 글자 분류, 텍스트 분류: 문서의 주제 분류, 유전자 데이터 분석

## 5. 최소 근접 (Nearest Neighbor) 알고리즘
- 직관적이고 간단한 모델이 필요한 경우.
- 데이터의 분포를 명확히 이해할 수 없거나, 비선형 관계가 존재할 때.
- 실시간 분류 작업(단, 데이터가 많을 경우 속도가 느려질 수 있음).
- ex)추천 시스템: 비슷한 사용자/상품 추천, 패턴 인식: 손글씨 숫자 분류, 의료 데이터 분석: 환자의 상태 분류.

## 6. 인공 신경망 (Neural Network)
- 데이터가 비선형적이고 복잡한 패턴을 가지고 있을 때.
- 대규모 데이터와 GPU를 사용할 수 있는 환경에서.
- 문제 해결에 기존 알고리즘보다 높은 성능이 필요할 때.
- ex)이미지 분류: 의료 영상에서 암 진단, 자연어 처리: 번역, 챗봇 개발, 음성 인식: 음성으로 명령 수행

## 7. 앙상블 (Ensemble)
- 다양한 모델을 결합하여 예측 성능을 높이고자 할 때.
- 단일 모델이 과적합되거나 성능이 낮은 경우.
- 데이터를 반복적으로 학습시켜 강력한 분류기를 만들고 싶을 때.
- ex)랜덤 포레스트: 금융 사기 탐지, 고객 이탈 분석. 그래디언트 부스팅(XGBoost, LightGBM): Kaggle 대회, 초고성능 모델 개발. 
배깅/부스팅: 의료 진단, 주가 예측.

## 8. 요약
- 단순한 데이터와 빠른 예측 → 나이브 베이즈, 로지스틱 회귀
- 해석 가능성과 직관 → 결정 트리
- 복잡한 데이터와 높은 정확도 → SVM, 신경망
- 간단하고 유연한 접근 → k-NN
- 고성능, 과적합 방지 → 앙상블


# 회귀 모델
## 1. 선형 회귀
1) 특징:
- 종속 변수와 독립 변수 간의 선형 관계를 모델링.
- 단순 선형 회귀(1개의 독립 변수)와 다중 선형 회귀(여러 독립 변수)로 구분.
2) 적합한 상황:
- 데이터가 선형적으로 분포되어 있는 경우.
- 관계를 설명하고 해석이 중요한 경우.
3) 예시:
- 주택 가격 예측 (평수, 방 개수 등과 가격 간의 선형 관계).
- 광고 비용과 매출 간의 관계 분석.

## 2. 다중 선형 회귀(+)
1) 특징
- 하나 이상의 독립 변수와 종속 변수 간의 선형 관계를 모델링.
- 변수들이 선형 독립적이고, 데이터가 선형적으로 분포되어 있는 경우에 적합.
2) 적합한 상황
- 독립 변수(특성)가 종속 변수에 직선 형태로 영향을 미칠 때.
- 해석 가능성과 간단한 모델이 필요한 경우.
3) 예시
- 주택 가격 예측: 방 개수, 위치, 면적 등의 독립 변수로 가격 예측.
- 마케팅 효과 분석: 광고비, 판매량 등 독립 변수와 매출의 관계.
- 학생 성적 예측: 출석률, 학습 시간과 시험 점수의 관계.

## 3. 다항 회귀
1) 특징
- 선형 회귀의 확장으로, 독립 변수의 다항식 항을 포함하여 비선형 데이터를 모델링.
- 데이터가 곡선 형태로 분포되어 있을 때 적합.
2) 적합한 상황
- 종속 변수와 독립 변수 간의 관계가 선형이 아니지만 특정 패턴(예: 곡선)이 있는 경우.
3) 예시
- 자동차 속도와 제동 거리 예측: 속도가 증가함에 따라 제동 거리가 곡선 형태로 증가.
- 온도 변화와 에너지 소비량 예측: 온도와 에너지 사용량 간의 비선형 관계.
- 인구 성장 모델링: 시간이 지남에 따라 인구가 곡선 형태로 증가.

## 4. 릿지 회귀
1) 특징:
- 선형 회귀의 변형으로, 모델의 과적합(overfitting)을 방지하기 위해 L2 정규화 추가.
2) 적합한 상황:
- 데이터에 다중 공선성(multicollinearity)이 존재하는 경우.
- 독립 변수의 수가 많아 과적합 위험이 높은 경우.
3) 예시:
- 고차원 데이터를 이용한 의료 비용 예측.
- 유사한 특성이 많은 변수 간의 연관성을 제거하고 정확한 예측.

## 5. 라쏘 회귀
1) 특징:
- 리지 회귀와 유사하지만, L1 정규화를 사용하여 중요하지 않은 변수의 가중치를 0으로 만들어 변수 선택(feature selection)을 수행.
2) 적합한 상황:
- 변수 선택이 필요한 경우.
- 데이터의 차원이 매우 높은 경우.
3) 예시:
- 금융 데이터에서 중요한 변수만 골라 대출 상환 예측.
- 생물학적 데이터에서 유전자와 특정 질병 간의 관계 분석

## 6. 엘라스틱넷 회귀
1) 특징:
- 리지 회귀와 라쏘 회귀를 결합한 알고리즘.
- L1과 L2 정규화를 동시에 적용.
2) 적합한 상황:
- 데이터가 희소(sparse)하거나, 독립 변수 간 상관관계가 있는 경우.
3) 예시:
- 유전자 데이터 분석에서 변수 선택 및 예측.
- 마케팅 데이터에서 주요 변수 도출과 매출 예측.

## 7. 서포트 벡터 회귀(+)
1) 특징:
- 서포트 벡터 머신(SVM)을 회귀 문제로 확장.
- 초평면을 중심으로 특정 마진 내의 오류를 허용하며 예측.
2) 적합한 상황:
- 데이터가 복잡하고 비선형 관계를 가질 때.
- 소규모 데이터 세트에서 높은 예측 성능이 필요할 때.
3) 예시:
- 주식 가격 예측.
- 온도 변화 예측.

## 8. 의사결정 회귀 트리(+)
1) 특징:
- 데이터를 계층적으로 분할하여 예측.
- 비선형 관계를 효과적으로 모델링 가능.
2) 적합한 상황:
- 데이터가 비선형적이고, 범주형/연속형 변수가 혼합되어 있는 경우.
3) 예시:
- 소비자 행동에 따른 매출 예측.
- 특정 날씨 조건에서 에너지 소비량 예측.

## 9. 랜덤 포레스트 회귀(+)
1) 특징:
- 여러 결정 트리의 예측을 결합해 평균값을 계산.
- 과적합 방지 및 높은 예측 성능 제공.
2) 적합한 상황:
- 고차원 데이터와 비선형적인 관계를 다룰 때.
- 안정적이고 강건한 예측이 필요할 때.
3) 예시:
- 농작물 수확량 예측.
- 제품 수요 예측.

## 10. 그래디언트 부스팅 회귀(+)
1) 특징:
- 부스팅(Boosting) 기법을 사용해 여러 약한 학습자를 결합하여 강력한 예측 모델 생성.
- XGBoost, LightGBM, CatBoost 등이 대표적인 알고리즘.
2) 적합한 상황:
- 높은 예측 성능이 요구되는 복잡한 문제.
- 대회(Kaggle 등)나 실제 서비스에 최적화된 모델이 필요한 경우.
3) 예시:
- 고객 평점 예측.
- 의료 비용 예측.

## 11. k-최근접 회귀(+)
1) 특징:
- 특정 데이터의 가장 가까운 k개의 이웃을 참조하여 예측.
2) 적합한 상황:
- 데이터가 비교적 간단하고, 비선형적인 관계를 보일 때.
3) 예시:
- 주택 가격 예측 (근처 유사 주택의 평균 가격 활용).
- 소매 매장 매출 예측.

## 12. 인공 신경망 회귀(+)
1) 특징:
- 다층 퍼셉트론(MLP)이나 심층 신경망(Deep Neural Network)을 사용해 복잡한 관계를 모델링.
- 데이터를 비선형적으로 다룰 수 있음.
2) 적합한 상황:
- 데이터가 방대한 양을 가지고 있고, 비선형적 관계를 포함.
- 고성능 GPU가 사용 가능한 경우.
3) 예시:
- 부동산 가격 예측.
- 날씨 패턴 기반 에너지 수요 예측.

## 13. 베이시안 회귀(+)
1) 특징:
- 예측값뿐만 아니라 예측의 신뢰 구간까지 제공.
- 확률론적 접근을 기반으로 한 회귀 알고리즘.
2) 적합한 상황:
- 불확실성이 중요한 문제에서 신뢰 구간을 함께 예측할 때.
3) 예시:
- 의료 진단에서 병 발병 가능성 추정.
- 신제품 판매량 예측에서 불확실성 모델링.

## 14. 요약
- 선형적 데이터 → 선형 회귀, 리지 회귀, 라쏘 회귀.
- 비선형적 데이터 → 다항 회귀, SVR, 결정 트리.
- 복잡한 데이터와 높은 성능 요구 → 랜덤 포레스트, 그래디언트 부스팅, 신경망.
- 변수 선택 → 라쏘 회귀, 엘라스틱넷.
- 불확실성 고려 → 베이지안 회귀.


# 군집화 모델
# 1. K-means
1) 특징
- 데이터를 𝑘개의 클러스터로 나누고, 각 클러스터의 중심(centroid)을 계산하여 데이터 포인트를 중심에 가장 가까운 클러스터에 할당.
- 클러스터 개수 k를 미리 지정해야 함.
- 데이터가 구형(원형) 클러스터 형태일 때 효과적.
2) 적합한 상황
- 클러스터 개수가 명확히 정의된 경우: k값을 미리 알고 있거나 추정할 수 있는 경우.
- 구형(원형) 클러스터 구조: 데이터의 각 클러스터가 서로 명확히 분리된 원형 형태일 때.
- 대규모 데이터: 계산이 상대적으로 빠르고 메모리 효율적.
3) 예시:
- 고객 세분화 (Customer Segmentation): 고객 특성 데이터를 사용해 그룹화.
- 이미지 압축: 색상 데이터의 군집화를 통해 비슷한 색을 합쳐 압축.

# 2. Mean Shift
1) 특징
- 밀도가 높은 지역(모드)을 중심으로 클러스터링. 클러스터의 개수를 미리 지정할 필요 없음.
- 대역폭 (bandwidth): 클러스터의 범위를 결정하는 핵심 하이퍼파라미터.
- 밀도가 높은 지역을 반복적으로 찾기 때문에 클러스터링이 비원형 데이터에도 적합.
2) 적합한 상황
- 클러스터 개수를 알 수 없는 경우: 데이터에서 클러스터 수가 자동으로 결정됨.
- 비선형 분포 데이터: 클러스터의 모양이 복잡한 형태인 경우.
- 밀도가 중요한 데이터: 데이터 밀도에 따라 클러스터가 결정되므로 밀도가 높은 지역을 찾는 문제에 적합.
3) 예시:
- 이미지 세그멘테이션: 이미지에서 색상이나 밝기를 기준으로 영역을 분할.
- 영상 추적 (Object Tracking): 동영상에서 객체의 움직임을 추적.
4) 참고
- estimate_bandwidth: 적절한 대역폭을 추정하여 클러스터링 성능을 높이는 데 사용.

# 3. GMM
1) 특징
- 데이터를 가우시안 분포의 혼합 모델로 가정하여 클러스터링.
- 각 클러스터를 하나의 가우시안 분포로 모델링.
- 확률적 군집화: 각 데이터 포인트가 특정 클러스터에 속할 확률을 계산.
- 클러스터 개수 k를 미리 지정해야 함.
2) 적합한 상황
- 클러스터 간의 모양이 타원형인 경우: 가우시안 분포로 데이터를 모델링하므로 비구형 클러스터링 가능.
- 확률 기반 군집화가 필요한 경우: 데이터 포인트가 여러 클러스터에 속할 가능성이 있는 경우.
- 데이터가 가우시안 분포를 따른다고 가정할 수 있을 때.
3) 예시:
- 고객 세분화: 고객의 구매 패턴이 여러 군집에 걸쳐 있을 수 있는 경우.
- 데이터 분포 추정: 밀도 추정 또는 데이터 생성 모델 학습.
- 잡음 제거 (Outlier Detection): 비정상 데이터를 가우시안 모델에서 벗어난 포인트로 감지.

# 4. DBSCAN
1) 특징
- 밀도 기반 군집화 알고리즘. 특정 반경 내에 데이터가 얼마나 밀집되어 있는지 기준으로 클러스터 형성.
- 클러스터 개수 k를 미리 지정할 필요 없음.
- 노이즈(outlier)를 자연스럽게 감지하여 클러스터에 포함시키지 않음.
2) 조정 가능한 파라미터
- eps: 데이터 포인트 간의 최대 거리 (클러스터의 밀도를 결정).
- min_samples: 클러스터를 형성하기 위한 최소 데이터 포인트 수.
3) 적합한 상황
- 클러스터 개수를 알 수 없는 경우: 데이터에서 밀도가 높은 영역을 클러스터로 간주.
- 비원형 데이터: 클러스터가 임의의 모양(선형, 비선형 등)을 가질 때.
- 잡음(outlier)이 많은 데이터: 잡음을 자연스럽게 제외하고 클러스터링 가능.
4) 예시:
- 지리 데이터 분석: 위치 데이터를 사용해 도시 내 밀집된 지역 식별.
- 이상치 탐지 (Outlier Detection): 이상치가 존재하는 데이터의 군집화.
- 교통 데이터 분석: 차량의 위치 데이터로 혼잡 구역 탐지.

![image](https://github.com/user-attachments/assets/0ed089a5-e40a-41d6-95ae-33ec89971b15)


# 5. 선택 기준
1. 클러스터 개수를 알고 있는가?
- 알고 있음: k-Means, GMM.
- 모름: Mean Shift, DBSCAN.

2. 클러스터의 형태
- 구형: k-Means.
- 비구형/복잡한 형태: DBSCAN, Mean Shift, GMM.

3. 잡음(outlier)의 존재
- 잡음 처리 필요: DBSCAN.
- 잡음 처리 필요 없음: k-Means, GMM.

4. 밀도가 중요한가?
- 밀도 기반 클러스터링: Mean Shift, DBSCAN.

*데이터 선형/비선형 비교 코드
*데이터 불러오기/확인 코드
*각각의 알고리즘 적용 코드
*분류/회귀/군집화에 대한 평가 코드
